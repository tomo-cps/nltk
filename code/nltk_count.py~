import nltk
import csv
import pandas as pd

from nltk import word_tokenize, pos_tag
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer
from collections import Counter


def get_wordnet_pos(tag):
#    if tag.startswith('J'):
#        return wordnet.ADJ
#    elif tag.startswith('V'):
#        return wordnet.VERB
    if tag.startswith('N'):
        return wordnet.NOUN
#    elif tag.startswith('R'):
#        return wordnet.ADV
    else:
        return None

lines = pd.read_csv('/Users/okawatomoaki/Documents/nltk/nltk.csv')
#for index, data in lines.iterrows():
#    print(index)
#    print(data)
#    print('---------------')


#print(lines)
counts=[]

#print(lines['comment'])

#print(lines.shape)
for line in lines['comment']:
#    print(i)

#for line in lines['comment'].tolist():
#    print(line)
        
        tokens = word_tokenize(line)
        tagged_sent = pos_tag(tokens)
        wordnet_pos_nn=[ (x,y)  for (x,y) in tagged_sent if y in ('NN')]
               
        wnl = WordNetLemmatizer()
        lemmas_sent = [] 
        for tag in wordnet_pos_nn:
            wordnet_pos = get_wordnet_pos(tag[1]) or wordnet.NOUN
            lem = wnl.lemmatize(tag[0],pos=wordnet_pos)
            lemmas_sent.append(lem)
        counts.append(dict(Counter(lemmas_sent)))
    #print(lemmas_sent)
print(counts)
